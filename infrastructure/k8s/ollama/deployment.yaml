apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: ai
spec:
  replicas: 1
  selector:
    matchLabels: { app: ollama }
  template:
    metadata:
      labels: { app: ollama }
    spec:
      initContainers:
      - name: ollama-pull-model
        image: ollama/ollama:latest
        # Using "command" is more explicit than "args" alone
        command: ["ollama"]
        args:
          - "pull"
          - "gemma2:2b-instruct"
        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
          limits:
            cpu: "1"
            memory: "1Gi"
        volumeMounts:
          - name: models
            mountPath: /root/.ollama
      containers:
      - name: ollama
        image: ollama/ollama:latest
        # --- THIS IS THE MAIN FIX ---
        # This new command starts the server and immediately pre-loads the model.
        command: ["/bin/sh", "-c"]
        args:
          - |
            ollama serve &
            PID=$!
            echo "Ollama server started in background with PID $PID"
            
            # Wait a few seconds for the server to initialize
            sleep 5
            
            echo "Pre-warming model: gemma2:2b-instruct..."
            # Send a silent request to the server to load the model into memory
            curl -s http://localhost:11434/api/generate -d '{ "model": "gemma2:2b-instruct", "prompt": "Pre-warm", "stream": false }' > /dev/null
            
            echo "Model is pre-warmed. The server is ready."
            
            # Wait for the background server process to finish
            wait $PID
        # --- END OF FIX ---
        ports:
          - containerPort: 11434
        env:
          - name: OLLAMA_MODELS
            value: /root/.ollama
          - name: OLLAMA_HOST
            value: "0.0.0.0"
        resources:
          requests:
            cpu: "1500m"
            memory: "6Gi"
          limits:
            cpu: "3000m"
            memory: "8Gi"
        volumeMounts:
          - name: models
            mountPath: /root/.ollama
        readinessProbe:
          httpGet: { path: /, port: 11434 } # Changed to a simpler health check path
          # Increased delay to give the model plenty of time to load into memory
          initialDelaySeconds: 60
          periodSeconds: 15
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: ollama-models
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: ai
spec:
  selector: { app: ollama }
  ports:
    - name: http
      port: 11434
      targetPort: 11434
      protocol: TCP
  type: ClusterIP